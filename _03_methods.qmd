# Methods

## Data Collection

The data for this capstone project encompass two main areas: social media and stock market. These data focus on three companies and one cryptocurrency. The companies are GameStop, Wingstop, and Nvidia, which are companies that have experienced recent notable growth or decline that could be reasonably attributed to social media movements. The one cryptocurrency, Dogecoin, has been a focus of X CEO Elon Musk. The goal, as mentioned prior, is to determine if it is possible to predict stock market motion using social media sentiment.

The stock market data was acquired via Yahoo Finance. This data is easily accessible. We acquired data for each of the organizations and pushed it into our database.

We used a PostgreSQL database hosted on Railway to store and access our data. This did cost us about $15 over the course of the term, but the tradeoff was worth it for its flexibility and the ability to have our data readily accessible to both of us at any time. We dumped our scraped social media data and stock market data directly into the database using the Python packages SQLAlchemy and psycopg2. We also used these packages to access the most up-to-date data during the data processing and modeling phase.

For social media data, we focused mainly on X and Reddit. On X, we aimed to scrape an average of 250 posts per topic per day for the designated time period. For Reddit, we scraped posts from the organization's subreddit, usually named after the company/currency; these subreddits were r/nvidia, r/gamestop, r/wingstop, and r/dogecoin. In the case of GameStop and Nvidia, Reddit also had subreddits dedicated to the stocks themselves -- r/GME and r/NVDA_Stock -- which we also scraped.

### Twitter / X Web Scraping

The X web scraper was the more labor-intensive scraper of the two. This is due to a couple of factors. Firstly, since its acquisition by Elon Musk in late 2022, X has imposed strict crackdowns on web scraping through the implementation of rate limits on non-verified accounts. These reading rate limits are reported as 500 posts per day for new accounts, 1,000 posts per day for unverified users, and 10,000 posts for verified accounts [@reuters], though we read well over the 1,000 post limit per day during the scraping phase without hitting a rate wall. Secondly, the X API -- through which we would be able to pull the necessary data -- is locked behind a paywall of $100 per month [@xapi]. The API does have a free level, but this is limited to posting only. This is infeasible for the scope of this project, and thus required us to devise alternate means of collecting the X data. 

The X data was acquired manually through a pair of scrapers: twscrape and a custom Selenium-based webscraper that we built from scratch. The twscrape Python package, written by GitHub user vladkens, works through the authorized X API [@vladkens]. We created around 10 accounts that we gave to twscrape, and it was able to acquire a large amount of data very quickly. The drawback of this is that the accounts it used were quickly rate-limited if not outright banned, requiring the constant creation of new accounts. Due to resource constraints, we opted for the slower but more reliable Selenium-based scraper.

Our Selenium-based scraper was designed bespoke for X. In order to avoid hitting the X rate limits, we took a single thread approach, rotating through the 5 X accounts that were not banned during the twscrape scraper phase. During our scraping period, we were fortunate to not encounter any changes to website structure, allowing the scraper to run reliably for long periods of time. Cycling through different accounts also allowed us to avoid running up against X's rate limits for the most part. For the newer accounts, we did encounter bot deterrents in the form of orbit-matching authentication tests. These often occurred when running the scraper from different internet connections. Though there may be some method to circumvent this measure, we used human intervention and stepped into to solve these when they appeared.

### Reddit Web Scraping

For the Reddit scraper, we had to get access to the developer API, which is free to access with a valid Reddit account. Once we had access, we used the PRAW (Python Reddit API Wrapper) package to pull submissions from the above-mentioned subreddits. A limitation of the Reddit API is that Reddit does not allow date filtering in the same way X does. On the other hand, the amount of metadata that can be accessed about each submission is vast. One of these pieces of metadata is date of posting, stored as a UTC string. We were able to pull many new and top posts that fell within a given UTC time range. The limitation on date filtering meant we were unable to pull all of the posts for the date range, but we were still able to scrape a fair number of posts from the subreddits for our analysis.

### Ethical Considerations

Many website developers implement rate limits for their users accessing their site through the main interface or through API endpoints. Reddit and X have rate limits implemented for their APIs as well as users just accessing the site for everyday use. As part of our web scraping effort, we wanted to ensure that we respected the rate limits as much as possible while still being able to collect the data we needed for our analysis. For the Reddit API, the free tier is limited to 100 queries per minute, which we did not come close to exceeding at only 6 queries in total. The X rate limit is a bit more complicated. Running our Selenium scraper on a single account for several queries in a row caused it to hit the rate limit after about 4 to 5 searches. However, by rotating the accounts, we gave enough of a "cool down" period to each account to make sure that rate limit was never exceeded. Additionally, we took a single-thread approach to web scraping, only allowing one iteration of it to run at any given time. This was considerably slower, but ensured we were able to scrape without putting undue strain on the website.

Because social media post data are intrinsically tied to the individuals who produced them, we wanted to make sure the data was as anonymized as possible. Thus, we did not collect any user data, and limited our relevant to data to information we could gather from the posts themselves: the date of posting, the content of the post, and the number of likes (for X) or net post score (for Reddit). Doing this alleviates any privacy concerns that may arise.

## Sentiment Analysis

The next phase in the project is sentiment analysis. In order to perform the sentiment analysis on social media text data, we used the Python package vaderSentiment, authored by C.J. Hutto and E.E. Gilbert [@hutto]. This package uses VADER (Valence Aware Dictionary and sEntiment Reasoner), a natural language processing tool specifically designed to work well with calculating a sentiment score from social media data, making it apt for this application. Before running our posts through the sentiment analyzer, we first cleaned the data by removing URLs, duplicates, and punctuation. We also converted all of the text into lowercase. We used the SentimentIntensityAnalyzer classifier from the vaderSentiment package to perform the sentiment analysis. Using the VADER model, we found the sentiment score for each individual post and averaged the sentiment scores for each day for each of the four topics of interest. This was done using a simple average, though we did experiment with using a weighted average on likes. This, however, did not result in significant model improvements.

## Modeling

The goal of this project was to build a forecasting model for stock closing price movement based on average daily sentiment. Before starting to model, we compared the average sentiment with "close" to determine if there was a significant Pearson's correlation between the sentiment scores and the measured closing prices. This was to give us an idea of the relationship between our variables of interest, though it did not affect our modeling approaches. 

For our model construction, we used the singular feature of average sentiment with a response variable called "change" which could take one of two values: "up" or "down". Each row is assigned a change value based on the following day's closing price. If the next row's closing price is greater than the current row's, the current row will be assigned "up". It would be assigned "down" if the reverse occurred. In the case that the closing price did not change from one day to the next, it was assigned the value "noChange", which we ignored in our logistic regression to make the response binary.

We added additional rows for the changes in following days; change2nd, change3rd, change4th, and change5th. For example, the change2nd column value in a given row would denote the change between the following day and the day after that, the "change from the 2nd day to the 3rd day." This same idea applies to the rest of the change columns. We added these columns in as additional avenues for modeling.

### Logistic Regression

To perform our logistic regression, we used Python's scikit-learn package. We performed primarily univariate logistic regression, focusing on the average sentiment score as our feature and the change columns as our responses. We built separate models for each of the change columns using the average sentiment. Our data were split into a 80/20 train/test split, with the predicted accuracy gauged against the test response values. The metrics for performance we used was accuracy.

We developed logistic regression models for each of the four stock entities, as well as a model that used all of the four datasets together, which we called our "general" model. The results of these models can be found in the Data section of this document.

### Decision Trees

We wanted to build a few models to determine the most predictive system. Another model we built involved decision trees. For these models, we use scikit-learn's DecisionTreeClassifier classifier. Again, these trees used a single feature (the average sentiment) and an individual response variable (the change) for each model. As before, these models were built with an 80/20 train/test split. The max depth of each tree was held at 3 layers, as we found this to result in the highest accuracy across the board. As in logistic regression, we used accuracy, precision, recall, and f1-score as our primary metrics of performance.

As with logistic regression, we built individual models for each of the four organizations for each of the change columns, as well as a general model consisting of all of the data. One notable difference between the decision tree model and the logistic regression model is the inclusion of the "noChange" response. There were very few instances of this variable occurring, and ultimately its inclusion in the dataset made little change. The ultimate goal of this work was to find which model was better. We used a McNemar test to determine which of these two models performed better.

In the next secion, we will discuss our database and the data we collected.